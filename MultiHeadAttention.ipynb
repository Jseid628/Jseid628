{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "        # for each head, we create a linear layer for query, key, and value\n",
    "        self.q_weights = [nn.Linear(hidden_dim, hidden_dim) for _ in range(self.num_heads)]\n",
    "        self.k_weights = [nn.Linear(hidden_dim, hidden_dim) for _ in range(self.num_heads)]\n",
    "        self.v_weights = [nn.Linear(hidden_dim, hidden_dim) for _ in range(self.num_heads)]\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim = -1) #Don't understand this line\n",
    "        self.linear = nn.Linear(num_heads*hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for x in X:\n",
    "            x_result = []\n",
    "            for head in range(self.num_heads):\n",
    "                #Applies affine transformation to input matrix x\n",
    "                q = self.q_weights[head](x)\n",
    "                k = self.k_weights[head](x)\n",
    "                v = self.v_weights[head](x)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: torch.Size([4])\n",
      "Input x: tensor([ 0.0817,  0.7986, -0.3188,  0.8823])\n",
      "linear.weight.shape: torch.Size([3, 4])\n",
      "linear.weight.T.shape: torch.Size([4, 3])\n",
      "linear.weight: Parameter containing:\n",
      "tensor([[-0.4003, -0.2235,  0.2863,  0.1269],\n",
      "        [ 0.0233,  0.0677, -0.3841, -0.3928],\n",
      "        [-0.4478,  0.2313,  0.3289,  0.1121]], requires_grad=True)\n",
      "Shape of y: torch.Size([3])\n",
      "y: tensor([ 0.2183, -0.0419,  0.3914], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Pass from a 4 dim input -> 3 dim output\n",
    "x = torch.randn(4)\n",
    "print(f'Shape of x: {x.shape}')\n",
    "print(f'Input x: {x}')\n",
    "\n",
    "\n",
    "#Affine transformation is carried out by linear layer\n",
    "linear = nn.Linear(4, 3)\n",
    "print(f'linear.weight.shape: {linear.weight.shape}')\n",
    "print(f'linear.weight.T.shape: {linear.weight.T.shape}')\n",
    "\n",
    "print(f'linear.weight: {linear.weight}')\n",
    "\n",
    "#y stores the result of the affine transformation\n",
    "y = linear(x)\n",
    "print(f'Shape of y: {y.shape}')\n",
    "print(f'y: {y}') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = linear(x)$-> $xW^t + b$\n",
    "\n",
    "For this computation $W =$ $\\begin{pmatrix}\n",
    "w_{00} & w_{10} & w_{20} & w_{30} \\\\\n",
    "w_{01} & w_{11} & w_{21} & w_{31} \\\\\n",
    "w_{02} & w_{12} & w_{22} & w_{32}\n",
    "\\end{pmatrix}$ and $x = [x_0, x_1, x_2, x_3]$ and the vector $W_i$ is $[w_{ki}]$ for $k\\in [0,3]$, eg, $W_0 = [w_{00}, w_{10}, w_{20}, w_{30}]$. In addition the bias vector $b$ is $[b_0, b_1, b_2]$ and $W^t$ is $\\begin{pmatrix}\n",
    "w_{00} & w_{01} & w_{02} \\\\\n",
    "w_{10} & w_{11} & w_{12} \\\\\n",
    "w_{20} & w_{21} & w_{22} \\\\\n",
    "w_{30} & w_{31} & w_{32}\n",
    "\\end{pmatrix}$. Note that $W_0^t$ is simply the first column of $W^t$. Pytorch computes the product $xW^t = [x\\cdot W_0^t, x\\cdot W_1^t, x\\cdot W_2^t]$. Adding the bias $b$ gives $$y = xW^t = [x\\cdot W_0^t + b_0, x\\cdot W_1^t + b_1, x\\cdot W_2^t + b_2].$$ In terms of the shapes of the matrices / vectors involved, mathematically, we have: $$(1, 3) = (1, 4)(4, 3) + (1, 3)$$ $$y \\quad=x\\quad  W^t +\\quad b$$ In terms of the pytorch shapes, we have \n",
    "\n",
    "$$\\text{torch.Size([3]) = torch.Size([4]) torch.Size([4, 3]) + torch.Size([3])}$$ \n",
    "$$y\\quad\\quad = \\quad\\quad x\\quad W^t\\quad + b$$\n",
    "\n",
    "Now suppose that the (mathematical) shape of $x$ were $(2, 4)$, that is, if $x$ were: \n",
    "$\\begin{pmatrix}\n",
    "x_{00} & x_{01}  & x_{02} & x_{03} \\\\\n",
    "x_{10} & x_{11} & x_{12} & x_{13} \n",
    "\\end{pmatrix}$. This is equivalent to a torch tensor with shape torch.size([2,4]). If the first row of $x$ is denoted $x_0$ and the second is denoted $x_1$, then we may write the product $y = xW^t +b$ as: $$\\begin{pmatrix}\n",
    "x_0W_{0}^t + b_0 & x_0W^t_{1} + b_1  & x_0W^t_2 +b_2 \\\\\n",
    " x_1 W_0^t+ b_0 & x_1W^t_1 + b_1 & x_1W^t_2 +b_2\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "x_{00} & x_{01}  & x_{02} & x_{03} \\\\\n",
    "x_{10} & x_{11} & x_{12} & x_{13} \n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "w_{00} & w_{01} & w_{02} \\\\\n",
    "w_{10} & w_{11} & w_{12} \\\\\n",
    "w_{20} & w_{21} & w_{22} \\\\\n",
    "w_{30} & w_{31} & w_{32}\n",
    "\\end{pmatrix} + b.$$ Thus the matrix of weights has shape torch.Size([3,4]) and the input matrix has shape torch.Size([2,4]). Since the transpose of the weight matrix is used, the shapes align as $(2,4) \\cdot (4,3) = (2,3)$. The bias matrix $b$ is 'broadcast' among the rows of the product since each element of the row is a dot product of that row's input (a single training example) and weights, thereby representing the input to a node in a higher layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origianl list (as tensor):\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "List after softmax:\n",
      " tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "softmax = nn.Softmax(dim = 1)\n",
    "list = [[1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 9]]\n",
    "\n",
    "list = torch.tensor(list, dtype=torch.float32)\n",
    "print(f'Origianl list (as tensor):\\n {list}')\n",
    "softmax_list = softmax(list)\n",
    "print(f'List after softmax:\\n {softmax_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our list tensor has two dimensions since it is two dimensional. We can select dim = 1 or 0 (or -1 = 1). Applying softmax along dim = 0 applies softmax along columns in our 2d array. Selecting dim = 1 applies softmax along rows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
