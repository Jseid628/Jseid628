{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "        # for each head, we create a linear layer for query, key, and value\n",
    "        self.q_weights = [nn.Linear(hidden_dim, hidden_dim) for _ in range(self.num_heads)]\n",
    "        self.k_weights = [nn.Linear(hidden_dim, hidden_dim) for _ in range(self.num_heads)]\n",
    "        self.v_weights = [nn.Linear(hidden_dim, hidden_dim) for _ in range(self.num_heads)]\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim = -1) #Don't understand this line\n",
    "        self.linear = nn.Linear(num_heads*hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for x in X:\n",
    "            x_result = []\n",
    "            for head in range(self.num_heads):\n",
    "                #Applies affine transformation to input matrix x\n",
    "                q = self.q_weights[head](x)\n",
    "                k = self.k_weights[head](x)\n",
    "                v = self.v_weights[head](x)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: torch.Size([4])\n",
      "Input x: tensor([-0.0198, -0.4477,  0.5764,  0.8019])\n",
      "linear.weight.shape: torch.Size([3, 4])\n",
      "linear.weight: Parameter containing:\n",
      "tensor([[-0.4017, -0.0048, -0.1204,  0.0214],\n",
      "        [ 0.0603,  0.3952,  0.1923, -0.1752],\n",
      "        [ 0.4260,  0.1708, -0.3359, -0.2139]], requires_grad=True)\n",
      "Shape of y: torch.Size([3])\n",
      "y: tensor([ 0.3387, -0.5468, -0.3428], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Pass from a 4 dim input -> 3 dim output\n",
    "x = torch.randn(4)\n",
    "print(f'Shape of x: {x.shape}')\n",
    "print(f'Input x: {x}')\n",
    "\n",
    "\n",
    "#Affine transformation is carried out by linear layer\n",
    "linear = nn.Linear(4, 3)\n",
    "print(f'linear.weight.shape: {linear.weight.shape}')\n",
    "print(f'linear.weight: {linear.weight}')\n",
    "\n",
    "#y stores the result of the affine transformation\n",
    "y = linear(x)\n",
    "print(f'Shape of y: {y.shape}')\n",
    "print(f'y: {y}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above shows that linear_layer(input_data) performs the following operation: $$((2, 4) \\cdot (4,1)) + (2, 1) = (2, 1).$$ A more detailed explanation of the same idea is the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t_weights = [nn.Linear(3, 3) for _ in range(2)] is a collection of 2 linear layers. Each linear layer is a matrix of weights plus a bias vector. The first weight matrix, t_weights[0], is: \n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "0.34 & 0.57 & -0.53 \\\\\n",
    "-0.03 & -0.53 & 0.42 \\\\\n",
    "-0.16 & 0.50 & 0.28\n",
    "\\end{pmatrix}\n",
    "$$ \n",
    "and the vector $x$ is simply [0, 0, 0]. The bias vector is [-0.50, -0.43, 0.402].\n",
    "\n",
    "These elements have shapes:\n",
    "\n",
    "- t_weights[0].weight.shape = torch.Size([3,3])\n",
    "- t_weights[0].bias.shape = torch.Size([3])\n",
    "- x.shape = torch.Size([1,3])\n",
    "\n",
    "The product t_weights[0].weights and x is simply \n",
    "$\n",
    "\\begin{pmatrix}\n",
    "0.34 & 0.57 & -0.53 \\\\\n",
    "-0.03 & -0.53 & 0.42 \\\\\n",
    "-0.16 & 0.50 & 0.28\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix}0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n",
    "$\n",
    "\n",
    "Adding the bias, we have t_weights[0] (x) = $ \n",
    "\\begin{pmatrix}\n",
    "0.34 & 0.57 & -0.53 \\\\\n",
    "-0.03 & -0.53 & 0.42 \\\\\n",
    "-0.16 & 0.50 & 0.28\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix}-0.50 \\\\ -0.43 \\\\ 0.402 \\end{pmatrix} = \\begin{pmatrix}-0.50 \\\\ -0.43 \\\\ 0.402 \\end{pmatrix}\n",
    "$\n",
    "\n",
    "represented by the tensor tensor([[-0.50, -0.43, 0.402]], grad_fn=<AddmmBackward0>) [Something is wrong here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also take the product of t_weights[i] with a matrix of shape $(3, 3)$. The result is $(3, 3) \\cdot (3, 3) = (3, 3)$ to which we add the bias of shape $(3, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list $r$ is a collection of three linear layers. Consider the first layer, $r[0]$ - it consists of a weight matrix $r[0].weight$, and a bias term, $r[0].bias$. The list $r_x$ consists of the linear layers $r[i]$ applied to $x$. The product of $x$ - $(3,1)$ -  with $r[0]$ - $(3,3)$ is a vector of shape $(3,1)$ to which we add the bias $(3,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product for the vectors $r[0](x)$ and $r[1](x)$ - both of shape (3, 1): rows, columns - is a scalar. We take the product of this scalar with @ again to obtain another (3,1) vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origianl list (as tensor):\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "List after softmax:\n",
      " tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "softmax = nn.Softmax(dim = 1)\n",
    "list = [[1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 9]]\n",
    "\n",
    "list = torch.tensor(list, dtype=torch.float32)\n",
    "print(f'Origianl list (as tensor):\\n {list}')\n",
    "softmax_list = softmax(list)\n",
    "print(f'List after softmax:\\n {softmax_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our list tensor has two dimensions since it is two dimensional. We can select dim = 1 or 0 (or -1 = 1). Applying softmax along dim = 0 applies softmax along columns in our 2d array. Selecting dim = 1 applies softmax along rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4494,  0.1101, -0.4059, -0.4762],\n",
      "        [-0.1340, -0.4289, -0.3559,  0.4126],\n",
      "        [-0.3729,  0.3202, -0.2928, -0.2099]], requires_grad=True)\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "in_features, out_features = (4, 3)\n",
    "y = nn.Linear(in_features, out_features)\n",
    "print(y.weight)\n",
    "print(y.weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9926, -0.9342, -0.5142,  1.2954])\n",
      "torch.Size([4])\n",
      "Weight shape: torch.Size([3, 4])\n",
      "Bias shape: torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0319,  0.4415, -0.2730,  0.0101],\n",
      "        [ 0.3719,  0.0518, -0.2535, -0.4235],\n",
      "        [ 0.4518, -0.1185, -0.1704,  0.2925]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4185,  0.3588, -0.4593], requires_grad=True)\n",
      "tensor([-0.7092, -0.4770, -0.3304], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input: 4-node layer output (vector with 4 entries)\n",
    "x = torch.randn(4)  # Shape (4,)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "# Linear layer: 4 inputs â†’ 3 outputs\n",
    "linear = nn.Linear(4, 3)\n",
    "\n",
    "# What PyTorch stores:\n",
    "print(f\"Weight shape: {linear.weight.shape}\")  # (3, 4) - not (4, 3)!\n",
    "print(f\"Bias shape: {linear.bias.shape}\")      # (3,)\n",
    "print(linear.weight)\n",
    "print(linear.bias)\n",
    "\n",
    "# Forward pass: y = x @ W^T + b\n",
    "y = linear(x)  # Shape (3,)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0319,  0.3719,  0.4518],\n",
      "        [ 0.4415,  0.0518, -0.1185],\n",
      "        [-0.2730, -0.2535, -0.1704],\n",
      "        [ 0.0101, -0.4235,  0.2925]], grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(linear.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Manual computation\n",
    "W = linear.weight      # (3, 4)\n",
    "b = linear.bias        # (3,)\n",
    "y_manual = x @ W.T + b # Same as linear(x)\n",
    "\n",
    "print(torch.allclose(y, y_manual))  # True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
