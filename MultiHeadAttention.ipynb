{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "        # for each head, we create a linear layer for query, key, and value\n",
    "        self.q_weights = [nn.Linear(hidden_dim, hidden_dim) for _ in range(self.num_heads)]\n",
    "        self.k_weights = [nn.Linear(hidden_dim, hidden_dim) for _ in range(self.num_heads)]\n",
    "        self.v_weights = [nn.Linear(hidden_dim, hidden_dim) for _ in range(self.num_heads)]\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.linear = nn.Linear(num_heads*hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for x in X:\n",
    "            x_result = []\n",
    "            for head in range(self.num_heads):\n",
    "                q = self.q_weights[head](x)\n",
    "                k = self.k_weights[head](x)\n",
    "                v = self.v_weights[head](x)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of 0th weight matrix in w: torch.Size([2, 4])\n",
      "shape of 0th bias matrix in w: torch.Size([2])\n",
      "---\n",
      "0th weight matrix: Parameter containing:\n",
      "tensor([[-0.1985, -0.1633,  0.3393,  0.0411],\n",
      "        [-0.1433, -0.1576,  0.1237,  0.1431]], requires_grad=True)\n",
      "0th bias matrix: Parameter containing:\n",
      "tensor([-0.1693,  0.0828], requires_grad=True)\n",
      "---\n",
      "[0, 0, 0, 0]\n",
      "torch.Size([4])\n",
      "---\n",
      "[tensor([-0.1693,  0.0828], grad_fn=<ViewBackward0>), tensor([ 0.1212, -0.1277], grad_fn=<ViewBackward0>), tensor([-0.3210,  0.1686], grad_fn=<ViewBackward0>)]\n",
      "num rows in results: 3\n",
      "num cols in results: 2\n"
     ]
    }
   ],
   "source": [
    "# A matrix like self.q_weights\n",
    "w = [nn.Linear(4, 2) for _ in range(3)]\n",
    "for i in range(len(w)):\n",
    "    print(f'shape of {i}th weight matrix in w: {w[i].weight.shape}')\n",
    "    print(f'shape of {i}th bias matrix in w: {w[i].bias.shape}')\n",
    "    if i == 0:\n",
    "        break\n",
    "print(\"---\")\n",
    "\n",
    "for i in range(len(w)):\n",
    "    print(f'{i}th weight matrix: {w[i].weight}')\n",
    "    print(f'{i}th bias matrix: {w[i].bias}')\n",
    "    if i == 0:\n",
    "        break\n",
    "print(\"---\")\n",
    "\n",
    "list = [0 for _ in range(4)]\n",
    "print(list)\n",
    "list = torch.tensor(list, dtype=torch.float32)\n",
    "print(list.shape)\n",
    "print('---')\n",
    "\n",
    "input_data = list\n",
    "results =[]\n",
    "for linear_layer in w:\n",
    "    output = linear_layer(input_data)\n",
    "    if linear_layer == 0:\n",
    "        print(output)\n",
    "        print(output.shape)\n",
    "    results.append(output)\n",
    "\n",
    "print(results)\n",
    "print(f'num rows in results: {len(results)}')\n",
    "print(f'num cols in results: {len(results[0])}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above shows that linear_layer(input_data) performs the following operation: $$((2, 4) \\cdot (4,1)) + (2, 1) = (2, 1).$$ A more detailed explanation of the same idea is the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of t_weights is: <class 'list'>\n",
      "The data type of t_weights[0] is: <class 'torch.nn.modules.linear.Linear'>\n",
      "\n",
      "The torch tensor x is tensor([[1., 1., 1.]]), and has shape torch.Size([1, 3])\n",
      "\n",
      "t_weights weights: Parameter containing:\n",
      "tensor([[ 0.1195,  0.3768,  0.3752],\n",
      "        [ 0.1162, -0.1320, -0.3679],\n",
      "        [-0.1940, -0.0523, -0.1250]], requires_grad=True)\n",
      "t_weights bias: Parameter containing:\n",
      "tensor([-0.3283,  0.0247,  0.1290], requires_grad=True)\n",
      "\n",
      "t_weights[i]: Linear(in_features=3, out_features=3, bias=True)\n",
      "t_weights[0](tensor([[1., 1., 1.]])): tensor([[ 0.5432, -0.3590, -0.2423]], grad_fn=<AddmmBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list of linear layers (two linear layers).\n",
    "t_weights = [nn.Linear(3, 3) for _ in range(2)]\n",
    "\n",
    "print(f'The data type of t_weights is: {type(t_weights)}')\n",
    "print(f'The data type of t_weights[0] is: {type(t_weights[0])}\\n')\n",
    "\n",
    "x = torch.tensor([[1.0, 1.0, 1.0]], dtype=torch.float32)\n",
    "print(f'The torch tensor x is {x}, and has shape {x.shape}\\n')\n",
    "\n",
    "for i in range(len(t_weights)):\n",
    "\n",
    "    print(f't_weights weights: {t_weights[i].weight}')\n",
    "    print(f't_weights bias: {t_weights[i].bias}\\n')\n",
    "\n",
    "    print(f't_weights[i]: {t_weights[i]}')\n",
    "    print(f't_weights[{i}]({x}): {t_weights[i](x)}\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t_weights = [nn.Linear(3, 3) for _ in range(2)] is a collection of 2 linear layers. Each linear layer is a matrix of weights plus a bias vector. The first weight matrix, t_weights[0], is: \n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "0.34 & 0.57 & -0.53 \\\\\n",
    "-0.03 & -0.53 & 0.42 \\\\\n",
    "-0.16 & 0.50 & 0.28\n",
    "\\end{pmatrix}\n",
    "$$ \n",
    "and the vector $x$ is simply [1, 1, 1]. The bias vector is [-0.50, -0.43, 0.402].\n",
    "\n",
    "These elements have shapes:\n",
    "\n",
    "- t_weights[0].weight.shape = torch.Size([3,3])\n",
    "- t_weights[0].bias.shape = torch.Size([3])\n",
    "- x.shape = torch.Size([1,3])\n",
    "\n",
    "The product t_weights[0] (x) is simply \n",
    "$\n",
    "\\begin{pmatrix}\n",
    "0.34 & 0.57 & -0.53 \\\\\n",
    "-0.03 & -0.53 & 0.42 \\\\\n",
    "-0.16 & 0.50 & 0.28\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE LINEAR LAYER r[0]\n",
      "r[0]: Linear(in_features=3, out_features=3, bias=True)\n",
      "r[0].weight: Parameter containing:\n",
      "tensor([[ 0.3664,  0.1359, -0.2147],\n",
      "        [ 0.2572,  0.0596, -0.4002],\n",
      "        [-0.0569, -0.1019, -0.2516]], requires_grad=True)\n",
      "r[0].bias: Parameter containing:\n",
      "tensor([ 0.4570, -0.4915, -0.1332], requires_grad=True)\n",
      "\n",
      "--------\n",
      "RESULT OF AFFINE TRANSFORMATIONS r[i](x):\n",
      "r_x[0]: tensor([[ 0.7446, -0.5749, -0.5436]], grad_fn=<AddmmBackward0>)\n",
      "r_x[1]: tensor([[-0.7463, -0.9879,  0.5693]], grad_fn=<AddmmBackward0>)\n",
      "r_x[2]: tensor([[-0.9665,  0.6212, -0.1540]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list of linear layers (three linear layers).\n",
    "r = [nn.Linear(3, 3) for _ in range(3)]\n",
    "\n",
    "print('THE LINEAR LAYER r[0]')\n",
    "for i in range(len(r)):\n",
    "    print(f'r[{i}]: {r[i]}')\n",
    "    print(f'r[{i}].weight: {r[i].weight}')\n",
    "    print(f'r[{i}].bias: {r[i].bias}\\n')\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "#Stores the results of the linear layers applied to the vector x\n",
    "r_x = [r[i](x) for i in range(len(r))]\n",
    "\n",
    "print(\"--------\\nRESULT OF AFFINE TRANSFORMATIONS r[i](x):\")\n",
    "# Prints the affine transformations r[i](x)\n",
    "for i in range(len(r)):\n",
    "    print(f'r_x[{i}]: {r_x[i]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list $r$ is a collection of three linear layers. Consider the first layer, $r[0]$ - it consists of a weight matrix $r[0].weight$, and a bias term, $r[0].bias$. The list $r_x$ consists of the linear layers $r[i]$ applied to $x$. The product of $x$ - $(3,1)$ -  with $r[0]$ - $(3,3)$ is a vector of shape $(3,1)$ to which we add the bias $(3,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_product = r[0](x) @ r[1](x): tensor([[-0.2973]], grad_fn=<MmBackward0>)\n",
      "---------\n",
      "\n",
      "vector_product: tensor([[-0.2973]], grad_fn=<MmBackward0>)\n",
      "r[2](x): tensor([[-0.9665,  0.6212, -0.1540]], grad_fn=<AddmmBackward0>)\n",
      "vector_product_2: tensor([[ 0.2873, -0.1847,  0.0458]], grad_fn=<MmBackward0>)\n",
      "---------\n",
      "\n",
      "r[0](x).shape: torch.Size([1, 3])\n",
      "vector_product_2.shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "#Multiplying vectors using @\n",
    "print(f'vector_product = r[{0}](x) @ r[{1}](x): {r[0](x) @ r[1](x).T}')\n",
    "vector_product = r[0](x) @ r[1](x).T\n",
    "print('---------\\n')\n",
    "\n",
    "vector_product_2 = vector_product @  r[2](x)\n",
    "print(f'vector_product: {vector_product}')\n",
    "print(f'r[2](x): {r[2](x)}')\n",
    "print(f'vector_product_2: {vector_product_2}')\n",
    "\n",
    "print('---------\\n')\n",
    "\n",
    "print(f'r[0](x).shape: {r[0](x).shape}')\n",
    "print(f'vector_product_2.shape: {vector_product_2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product for the vectors $r[0](x)$ and $r[1](x)$ - both of shape (3, 1): rows, columns - is a scalar. We take the product of this scalar with @ again to obtain another (3,1) vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origianl list (as tensor):\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "List after softmax:\n",
      " tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "softmax = nn.Softmax(dim = 1)\n",
    "list = [[1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 9]]\n",
    "\n",
    "list = torch.tensor(list, dtype=torch.float32)\n",
    "print(f'Origianl list (as tensor):\\n {list}')\n",
    "softmax_list = softmax(list)\n",
    "print(f'List after softmax:\\n {softmax_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our list tensor has two dimensions since it is two dimensional. We can select dim = 1 or 0 (or -1 = 1). Applying softmax along dim = 0 applies softmax along columns in our 2d array. Selecting dim = 1 applies softmax along rows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
