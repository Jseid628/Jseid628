{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "        # for each head, we create a linear layer for query, key, and value\n",
    "        self.q_weights = [nn.Linear(hidden_dim, hidden_dim) for _ in range(self.num_heads)]\n",
    "        self.k_weights = [nn.Linear(hidden_dim, hidden_dim) for _ in range(self.num_heads)]\n",
    "        self.v_weights = [nn.Linear(hidden_dim, hidden_dim) for _ in range(self.num_heads)]\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.linear = nn.Linear(num_heads*hidden_dim, hidden_dim)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A matrix like self.q_weights\n",
    "w = [nn.Linear(4, 2) for _ in range(3)]\n",
    "for i in range(len(w)):\n",
    "    print(f'shape of {i}th weight matrix in w: {w[i].weight.shape}')\n",
    "    print(f'shape of {i}th bias matrix in w: {w[i].bias.shape}')\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in range(len(w)):\n",
    "    print(f'{i}th weight matrix: {w[i].weight}')\n",
    "    print(f'{i}th bias matrix: {w[i].bias}')\n",
    "    print(\"---\")\n",
    "\n",
    "list = [0 for _ in range(4)]\n",
    "print(list)\n",
    "list = torch.tensor(list, dtype=torch.float32)\n",
    "list.shape\n",
    "\n",
    "input_data = list\n",
    "results =[]\n",
    "for linear_layer in w:\n",
    "    output = linear_layer(input_data)\n",
    "    print(output)\n",
    "    print(output.shape)\n",
    "    results.append(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above shows that linear_layer(input_data) performs the following operation: $$((2, 4) \\cdot (4,1)) + (2, 1) = (2, 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "softmax = nn.Softmax(dim = 0)\n",
    "list = [[1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 9]]\n",
    "\n",
    "list = torch.tensor(list, dtype=torch.float32)\n",
    "print(f'Origianl list (as tensor):\\n {list}')\n",
    "softmax_list = softmax(list)\n",
    "print(f'List after softmax:\\n {softmax_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our list tensor has two dimensions since it is two dimensional. We can select dim = 1 or 0 (or -1 = 1). Applying softmax along dim = 0 applies softmax along columns in our 2d array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list of linear layers (two linear layers).\n",
    "# Each linear layer consists of a (3,3) weight matrix and a (3, 1) bias vector\n",
    "t_weights = [nn.Linear(3, 3) for _ in range(2)]\n",
    "\n",
    "print(f'The data type of t_weights is: {type(t_weights)}')\n",
    "print(f'The data type of t_weights[0] is: {type(t_weights[0])}\\n')\n",
    "\n",
    "x = torch.tensor([[1.0, 1.0, 1.0]], dtype=torch.float32)\n",
    "print(f'The torch tensor x is {x}, and has shape {x.shape}\\n')\n",
    "\n",
    "for i in range(len(t_weights)):\n",
    "\n",
    "    print(f't_weights weights: {t_weights[i].weight}')\n",
    "    print(f't_weights bias: {t_weights[i].bias}\\n')\n",
    "\n",
    "    print(f't_weights[i]: {t_weights[i]}')\n",
    "    print(f't_weights[{i}]({x}): {t_weights[i](x)}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t_weights = [nn.Linear(3, 3) for _ in range(2)] is a collection of 2 linear layers. Each linear layer is a matrix of weights plus a bias vector. The first weight matrix, t_weights[0], is: \n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "0.34 & 0.57 & -0.53 \\\\\n",
    "-0.03 & -0.53 & 0.42 \\\\\n",
    "-0.16 & 0.50 & 0.28\n",
    "\\end{pmatrix}\n",
    "$$ \n",
    "and the vector $x$ is simply [1, 1, 1]. The bias vector is [-0.50, -0.43, 0.402].\n",
    "\n",
    "These elements have shapes:\n",
    "\n",
    "- t_weights[0].weight.shape = torch.Size([3,3])\n",
    "- t_weights[0].bias.shape = torch.Size([3])\n",
    "- x.shape = torch.Size([1,3])\n",
    "\n",
    "The product t_weights[0] (x) is simply \n",
    "$\n",
    "\\begin{pmatrix}\n",
    "0.34 & 0.57 & -0.53 \\\\\n",
    "-0.03 & -0.53 & 0.42 \\\\\n",
    "-0.16 & 0.50 & 0.28\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n",
    "$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
