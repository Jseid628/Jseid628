{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "input_size = 784  # 28x28 pixels (not directly used in CNN)\n",
    "num_classes = 10  # digits 0-9\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10  # Reduced for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [(1, 2), (2, 3), (3, 4)]\n",
    "#Enumerate creates pairs of index / value.\n",
    "for a, b in enumerate(list):\n",
    "    print(a)\n",
    "\n",
    "for a, b in enumerate(list):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloaded the dataset right to Jseid628 folder\n",
    "train_dataset = datasets.MNIST(root=\"dataset/\", download=True, train=True, transform=transforms.ToTensor())\n",
    "#The DataLoader takes a data set and a batch size and creates an iterable (\"train_loader\" here). Each element of the iterable is a batch\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root=\"dataset/\", download=True, train=False, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx} | Data shape: {data.shape}, Targets Shape: {targets.shape}\")\n",
    "    if batch_idx == 0:  # Just show first batch\n",
    "        break\n",
    "\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test loader consists of many batches of 64 28*28 images (each x in test_loader is such a batch) and the labels of those images (each y in test_loader is a list of correponding labels)\n",
    "for x, y in test_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x[0].shape)\n",
    "    print(\"x.size(0): \" + str(x.size(0)))\n",
    "    print(\"y:\" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network Definition\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes = 10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #The pooling layer downsamples by half each time. Initially 28*28 -> 14*14 -> 7*7 due to the kernel size of 2 and stride 2.\n",
    "        self.fc1 = nn.Linear(16*7*7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "       \n",
    "        x = x.reshape(x.shape[0], -1) #flattens x into shape (batch_size, 16*7*7)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "        \n",
    "model = CNN(in_channels=1, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First layer kernels\n",
    "first_conv_layer = model.conv1\n",
    "kernels = first_conv_layer.weight.data.clone()\n",
    "print(kernels[0])\n",
    "print(f'kernel shape: {kernels.shape}')\n",
    "\n",
    "#Second layer kernels\n",
    "second_conv_layer = model.conv2\n",
    "kernels2 = second_conv_layer.weight.data.clone()\n",
    "print(f'kernel shape: {kernels2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if ((epoch % 1)== 0):\n",
    "        print(f\"Epoch {epoch} Loss: {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            outputs = model(x)\n",
    "            #print(len(outputs)) #should be 64 (batch size)\n",
    "            max_values, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "    \n",
    "        accuracy = float(num_correct) / float(num_samples) * 100\n",
    "        print(f\"Got {num_correct}/{num_samples} with accuracy {accuracy:.2f}%\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "check_accuracy(test_loader, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
